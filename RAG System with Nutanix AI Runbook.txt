RAG System with Nutanix AI Endpoints
This repository contains a Retrieval Augmented Generation (RAG) system designed to answer questions based on a corpus of PDF documents. It leverages Nutanix AI inference endpoints for both text embeddings and Large Language Model (LLM) inference, with an interactive Streamlit web interface.
This runbook provides precise, step-by-step instructions. Please follow each step exactly to ensure a smooth setup and flawless operation.
Table of Contents
1.	Overview
2.	Prerequisites
3.	Project Structure
4.	Step-by-Step Workflow Guide
o	Phase 1: Environment Setup & API Key Configuration
o	Phase 2: Data Ingestion & Vector Database Creation
o	Phase 3: Run the RAG Web Application
5.	Troubleshooting Common Issues
1. Overview
This project sets up a RAG system that allows you to chat with your PDF documents.
â€¢	PDF Processing: Extracts text from PDFs and chunks it.
â€¢	Embeddings: Uses the Nutanix AI Granite Embedding API (embedcpu model) to convert text chunks into numerical embeddings.
â€¢	Vector Database: Stores embeddings and document chunks in a local ChromaDB.
â€¢	LLM Inference: Utilizes the Nutanix AI Chat Completions API (mobileyedemo model) for generating responses.
â€¢	Frontend: Provides an interactive web interface built with Streamlit.
2. Prerequisites
Ensure you have the following installed on your Windows 11 system:
â€¢	Anaconda or Miniconda (Recommended): This is crucial for managing Python environments and dependencies robustly. If you have an existing Anaconda/Miniconda installation and encounter issues, a clean re-installation is highly recommended (see Troubleshooting).
â€¢	Visual Studio Code (VS Code): Install with the Python Extension and Jupyter Extension for seamless development and notebook execution.
â€¢	Nutanix AI Account & API Key: Obtain your API key from the Nutanix AI platform. This single key is used for both embedding and LLM inference endpoints.
3. Project Structure
Organize your project directory as follows. You will create some of these files/folders, others will be generated.
your_project_folder/
â”œâ”€â”€ environment.yml             # Conda environment definition (YOU CREATE THIS)
â”œâ”€â”€ nai_api_key.txt             # Your API key (FOR YOUR REFERENCE ONLY - DO NOT COMMIT!)
â”œâ”€â”€ ingest_data.py              # Script to process PDFs and build the vector DB (YOU CREATE THIS)
â”œâ”€â”€ rag_core.py                 # Core RAG functions (imported by Streamlit app) (YOU CREATE THIS)
â”œâ”€â”€ streamlit_app.py            # The Streamlit web application frontend (YOU CREATE THIS)
â”œâ”€â”€ my_vector_db/               # Directory containing the ChromaDB (GENERATED by ingest_data.py)
â”‚   â””â”€â”€ (ChromaDB files)
â””â”€â”€ data/                       # Holds your source documents
    â””â”€â”€ pdfs/                   # Place your PDF documents here (YOU PLACE PDFs HERE)
        â””â”€â”€ (Your PDF documents e.g., document1.pdf, report.pdf)
â”œâ”€â”€ source_web_scrape.ipynb     # Jupyter notebook for scraping PDFs (OPTIONAL, if you need it)
â””â”€â”€ rag_system_full_workflow.ipynb # Original consolidated workflow (FOR REFERENCE/TESTING ONLY)

4. Step-by-Step Workflow Guide
Follow these phases and steps precisely.
Phase 1: Environment Setup & API Key Configuration
This phase ensures your Python environment is correctly set up and your API key is ready.
a. Create environment.yml file:
In your project's root directory, create a file named environment.yml (or environment.yaml) and paste the following content into it:
# environment.yml
name: rag_nutanix_env
channels:
  - conda-forge # Prioritize conda-forge for better package availability and recent versions
  - defaults    # Keep defaults as a fallback
dependencies:
  - python=3.10 # Explicit Python version. Using 3.10 for broad compatibility.
  - pip
  - requests
  - chromadb
  - pdfplumber
  - streamlit
  - pip:       # Use pip section for packages that might have better pip compatibility
    - langchain
    - langchain-community
    - langchain-openai

b. Create and Activate the Conda Environment:
1.	Close all instances of VS Code to ensure no lingering processes interfere.
2.	Open your Anaconda Prompt (search for it in the Windows Start Menu). This specific terminal is pre-configured to find conda.
3.	Navigate to your project directory in the Anaconda Prompt:
4.	cd "C:\Users\JoshStevenson\OneDrive - Nutanix\Technical\AI_Master\AI_APP_PROJECT"

(Important: Adjust this path to your actual project folder location).
5.	Install mamba (for faster and more reliable environment creation):
6.	conda install -n base conda-libmamba-solver mamba -c conda-forge

Respond y if prompted to proceed. This step makes conda use the faster libmamba solver.
7.	Clean Conda Cache: This helps prevent issues from corrupted metadata.
8.	conda clean --all

Respond y if prompted.
9.	Create the Project Environment using mamba:
10.	mamba env create -f environment.yml

This command reads your environment.yml and creates the rag_nutanix_env environment. This step might take a few minutes for dependency resolution and package downloads, but it is typically much faster than conda alone. Be patient.
11.	Activate the New Environment:
12.	conda activate rag_nutanix_env

Verify that your prompt now starts with (rag_nutanix_env), e.g., (rag_nutanix_env) C:\Users\JoshStevenson\....
c. Configure API Key in Your Terminal Session:
Your Python scripts will read the NUTANIX_API_KEY from your environment variables. It's crucial to set it in the terminal session before launching any applications from that session.
1.	In the same Anaconda Prompt window where (rag_nutanix_env) is active, set your API key:
2.	set NUTANIX_API_KEY=YOUR_ACTUAL_NUTANIX_API_KEY_HERE

(Replace YOUR_ACTUAL_NUTANIX_API_KEY_HERE with your real Nutanix AI API key! There should be no spaces around the = sign.)
3.	Verify it's set for this session:
4.	echo %NUTANIX_API_KEY%

(It should print your API key.)
d. Launch VS Code from Activated Environment & Select Interpreter:
1.	From the same Anaconda Prompt window where (rag_nutanix_env) is active and your NUTANIX_API_KEY is set, launch VS Code:
2.	code .

This command opens your current directory in VS Code and ensures VS Code inherits the correct environment variables and PATH.
3.	In VS Code, open a new integrated terminal (Terminal > New Terminal). This new terminal should show (rag_nutanix_env) at its prompt.
4.	Select the Python Interpreter in VS Code:
o	In the bottom-left corner of VS Code, click on the Python interpreter name (it might say "Python 3.10.x" or similar).
o	From the list that appears, select Python 3.x.x ('rag_nutanix_env').
Phase 2: Data Ingestion & Vector Database Creation
This phase prepares your PDF corpus and builds the vector database. You run this once (or whenever your source PDFs change).
a. Prepare Source PDF Documents:
1.	Place all your PDF documents into the data/pdfs/ folder within your project directory.
2.	(Optional: If you need to scrape documents, open and run source_web_scrape.ipynb and ensure it saves PDFs to data/pdfs/.)
b. Create ingest_data.py script:
In your project's root directory, create a file named ingest_data.py and paste the following code:
import os
import shutil
import requests
import json

# LangChain Imports
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import Chroma

# --- Custom Embeddings Class for Nutanix Granite Embedding API ---
class NutanixGraniteEmbeddings(Embeddings):
    def __init__(self, api_key: str, endpoint_url: str, model_name: str):
        self.api_key = api_key
        self.endpoint_url = endpoint_url
        self.model_name = model_name
        self.session = requests.Session()

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self._get_embeddings(texts)

    def embed_query(self, text: str) -> list[float]:
        return self._get_embeddings([text])[0]

    def _get_embeddings(self, texts: list[str]) -> list[list[float]]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "accept": "application/json",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model_name,
            "input": texts,
            "encoding_format": "float"
        }

        response = None
        try:
            response = self.session.post(self.endpoint_url, headers=headers, json=payload)
            response.raise_for_status()
            result = response.json()

            if "data" in result and len(result["data"]) > 0:
                embeddings = [item["embedding"] for item in result["data"]]
                return embeddings
            else:
                raise ValueError("API response did not contain expected 'data' or embeddings.")

        except requests.exceptions.RequestException as e:
            print(f"API request failed: {e}")
            if response is not None:
                print(f"Response status: {response.status_code}")
                print(f"Response text: {response.text}")
            raise
        except json.JSONDecodeError:
            print(f"Failed to decode JSON from API response: {e}")
            raise
        except Exception as e:
            print(f"An unexpected error occurred during embedding: {e}")
            raise

def ingest_data_to_vector_db():
    print("--- Starting Data Ingestion Workflow ---")

    # --- Configuration ---
    PDF_DIR = "data/pdfs"
    CHROMA_DB_DIR = "my_vector_db"

    # Nutanix AI API Configuration for Embeddings
    EMBEDDING_API_MODEL_NAME = "embedcpu"
    NUTANIX_GRANITE_EMBEDDING_ENDPOINT = "https://ai.nutanix.com/api/v1/embeddings"
    
    # IMPORTANT: NUTANIX_API_KEY is read from environment variable.
    NUTANIX_API_KEY = os.getenv("NUTANIX_API_KEY")

    if not NUTANIX_API_KEY:
        print("WARNING: NUTANIX_API_KEY environment variable not set.")
        print("Please set this environment variable for the Nutanix AI APIs to work.")
        exit("API Key for Nutanix AI not found. Exiting.")

    # --- 1. Prepare Directories ---
    print("\n--- 1. Preparing Directories ---")
    os.makedirs(PDF_DIR, exist_ok=True)
    print(f"Ensuring PDF directory exists: {PDF_DIR}")

    # **CRITICAL:** Manual cleanup due to Windows PermissionError.
    # Ensure no other process (like a lingering Python/Jupyter kernel) is accessing this folder.
    if os.path.exists(CHROMA_DB_DIR):
        try:
            print(f"Attempting to remove existing ChromaDB data from: {CHROMA_DB_DIR}")
            shutil.rmtree(CHROMA_DB_DIR)
            print("Existing ChromaDB data removed successfully.")
        except PermissionError as e:
            print(f"PermissionError: Could not remove {CHROMA_DB_DIR}. Please close any programs accessing it and delete it manually.")
            print(f"Error details: {e}")
            exit("Cannot proceed without clearing old database. Please delete 'my_vector_db' manually or change CHROMA_DB_DIR.")
        except Exception as e:
            print(f"An unexpected error occurred while trying to remove {CHROMA_DB_DIR}: {e}")
            exit("Cannot proceed due to database cleanup error.")

    os.makedirs(CHROMA_DB_DIR, exist_ok=True)
    print(f"Ensuring ChromaDB directory exists: {CHROMA_DB_DIR}")


    # --- 2. Load PDF Documents ---
    print(f"\n--- 2. Loading PDF Documents from '{PDF_DIR}' ---")
    documents = []
    for filename in os.listdir(PDF_DIR):
        if filename.endswith(".pdf"):
            filepath = os.path.join(PDF_DIR, filename)
            try:
                loader = PyPDFLoader(filepath)
                documents.extend(loader.load())
                print(f"Loaded '{filename}'")
            except Exception as e:
                print(f"Error loading '{filename}': {e}")

    if not documents:
        print(f"No PDF documents found in '{PDF_DIR}'. Please place some PDFs there.")
        print("Example: You can create a dummy PDF file for testing or download one.")
        exit("No PDFs found. Exiting.")


    # --- 3. Split Documents into Chunks ---
    print("\n--- 3. Splitting Documents into Chunks ---")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=100) # 400 chars is ~100-150 tokens, safe for 512-token limit
    chunks = text_splitter.split_documents(documents)
    print(f"Original documents loaded: {len(documents)}")
    print(f"Documents split into {len(chunks)} chunks.")
    if chunks:
        print(f"First chunk example (first 200 chars):\n'{chunks[0].page_content[:200]}'")
    else:
        exit("No chunks generated from documents. Exiting.")


    # --- 4. Create Embeddings Model Instance (Nutanix Granite API) ---
    print(f"\n--- 4. Creating Embeddings using Nutanix Granite Embedding API ('{EMBEDDING_API_MODEL_NAME}') ---")
    try:
        embeddings_model = NutanixGraniteEmbeddings(
            api_key=NUTANIX_API_KEY,
            endpoint_url=NUTANIX_GRANITE_EMBEDDING_ENDPOINT,
            model_name=EMBEDDING_API_MODEL_NAME
        )
        dummy_embedding = embeddings_model.embed_query("test connection")
        print(f"Nutanix Granite Embedding API connected successfully. Embedding dimension: {len(dummy_embedding)}")
    except Exception as e:
        print(f"Error initializing or testing Nutanix Granite Embedding API: {e}")
        print("Please check your NUTANIX_API_KEY, endpoint URL, and internet connection.")
        exit()


    # --- 5. Create and Populate the Vector Database (ChromaDB) ---
    print("\n--- 5. Creating and Populating the Vector Database (ChromaDB) ---")
    try:
        vectorstore = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings_model,
            persist_directory=CHROMA_DB_DIR
        )
        vectorstore.persist()
        print(f"Vector database created and populated with {len(chunks)} chunks.")
        print(f"ChromaDB persisted to '{CHROMA_DB_DIR}'")
    except Exception as e:
        print(f"Error creating/populating ChromaDB: {e}")
        exit("Failed to create or populate the vector database.")

    print("--- Data Ingestion Workflow Complete ---")

if __name__ == "__main__":
    ingest_data_to_vector_db()

c. Create rag_core.py script:
In your project's root directory, create a file named rag_core.py and paste the following code:
import os
import requests
import json

from langchain_core.embeddings import Embeddings
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

# --- Custom Embeddings Class for Nutanix Granite Embedding API ---
# This class needs to be defined here as well because ChromaDB requires
# the same embedding function to load a persistent database.
class NutanixGraniteEmbeddings(Embeddings):
    def __init__(self, api_key: str, endpoint_url: str, model_name: str):
        self.api_key = api_key
        self.endpoint_url = endpoint_url
        self.model_name = model_name
        self.session = requests.Session()

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self._get_embeddings(texts)

    def embed_query(self, text: str) -> list[float]:
        return self._get_embeddings([text])[0]

    def _get_embeddings(self, texts: list[str]) -> list[list[float]]:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "accept": "application/json",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model_name,
            "input": texts,
            "encoding_format": "float"
        }

        response = None
        try:
            response = self.session.post(self.endpoint_url, headers=headers, json=payload)
            response.raise_for_status()
            result = response.json()

            if "data" in result and len(result["data"]) > 0:
                embeddings = [item["embedding"] for item in result["data"]]
                return embeddings
            else:
                raise ValueError("API response did not contain expected 'data' or embeddings.")

        except requests.exceptions.RequestException as e:
            print(f"API request failed: {e}")
            if response is not None:
                print(f"Response status: {response.status_code}")
                print(f"Response text: {response.text}")
            raise
        except json.JSONDecodeError:
            print(f"Failed to decode JSON from API response: {e}")
            raise
        except Exception as e:
            print(f"An unexpected error occurred during embedding: {e}")
            raise

# --- RAG System Core Functions ---

# Configuration (needs to be consistent with ingest_data.py and environment variables)
CHROMA_DB_DIR = "my_vector_db"
EMBEDDING_API_MODEL_NAME = "embedcpu"
NUTANIX_GRANITE_EMBEDDING_ENDPOINT = "https://ai.nutanix.com/api/v1/embeddings"
LLM_API_MODEL_NAME = "mobileyedemo" # Changed to "mobileyedemo"
NUTANIX_LLM_ENDPOINT = "https://ai.nutanix.com/api/v1"

# Global variables to store initialized components
_rag_chain = None
_vectorstore = None

def _initialize_rag_components():
    """Initializes and returns the RAG chain and vectorstore.
    Components are initialized only once per application run (Streamlit session).
    """
    global _rag_chain, _vectorstore

    if _rag_chain is not None: # Check if components are already initialized
        return _rag_chain, _vectorstore

    # Ensure API Key is set
    NUTANIX_API_KEY = os.getenv("NUTANIX_API_KEY")
    if not NUTANIX_API_KEY:
        raise ValueError("NUTANIX_API_KEY environment variable not set. Please set it.")

    # 1. Load the Persistent ChromaDB
    print(f"\n--- Loading ChromaDB from '{CHROMA_DB_DIR}' ---")
    embeddings_model = NutanixGraniteEmbeddings(
        api_key=NUTANIX_API_KEY,
        endpoint_url=NUTANIX_GRANITE_EMBEDDING_ENDPOINT,
        model_name=EMBEDDING_API_MODEL_NAME
    )
    # Ensure the database exists before trying to load it
    if not os.path.exists(CHROMA_DB_DIR):
        raise FileNotFoundError(f"ChromaDB directory '{CHROMA_DB_DIR}' not found. Please run ingest_data.py first.")
    _vectorstore = Chroma(persist_directory=CHROMA_DB_DIR, embedding_function=embeddings_model)
    print("ChromaDB loaded successfully.")

    # 2. Initialize the Remote LLM (Nutanix AI Endpoint)
    print(f"\n--- Initializing Remote Language Model (Nutanix AI Endpoint: '{LLM_API_MODEL_NAME}') ---")
    llm = ChatOpenAI(
        model=LLM_API_MODEL_NAME,
        temperature=0.0,
        openai_api_key=NUTANIX_API_KEY,
        base_url=NUTANIX_LLM_ENDPOINT
    )
    print(f"Remote LLM '{llm.model_name}' initialized successfully.")

    # 3. Create a Retriever from the Vector Database
    retriever = _vectorstore.as_retriever(search_kwargs={"k": 3})

    # 4. Build the RAG Chain
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Answer the user's question based on the provided context only. Respond concisely and in English.\nContext: {context}"),
        ("user", "{input}"),
    ])
    document_chain = create_stuff_documents_chain(llm, prompt)
    _rag_chain = create_retrieval_chain(retriever, document_chain)
    print("RAG chain built successfully.")
    
    return _rag_chain, _vectorstore

def ask_rag_system(query: str) -> dict:
    """
    Asks the RAG system a question and returns the LLM's answer and retrieved context.
    Components are initialized on first call.
    """
    try:
        rag_chain, _ = _initialize_rag_components()
        response = rag_chain.invoke({"input": query})
        return response
    except Exception as e:
        # Re-raise with a more informative message if needed, or handle specifically
        print(f"Error during RAG query: {e}")
        raise # Propagate the error up to Streamlit app

# Example usage (for testing this module directly)
if __name__ == "__main__":
    print("Running rag_core.py as main for testing...")
    os.environ["NUTANIX_API_KEY"] = "YOUR_ACTUAL_NUTANIX_API_KEY_HERE" # REMOVE IN PROD!
    try:
        test_query = "What is the main topic of these documents?"
        print(f"\nTesting with query: '{test_query}'")
        response = ask_rag_system(test_query)
        print("\nTest LLM Answer:")
        print(response["answer"])
        print("\nTest Retrieved Context (Documents used by LLM):")
        if response["context"]:
            for j, doc in enumerate(response["context"]):
                print(f"  - Document {j+1} (Source: {doc.metadata.get('source', 'N/A')}, Page: {doc.metadata.get('page', 'N/A')})")
                print(f"    Content (first 100 chars): {doc.page_content[:100]}...")
        else:
            print("  No relevant documents were retrieved for the test query.")
    except Exception as e:
        print(f"Error during rag_core test: {e}")

d. Create streamlit_app.py script:
In your project's root directory, create a file named streamlit_app.py and paste the following code:
import streamlit as st
import rag_core # Import our RAG core functions

st.set_page_config(page_title="RAG Chatbot", layout="centered")

st.title("ðŸ“„ Document Q&A with RAG")
st.markdown("""
Welcome! Ask a question about the documents in our knowledge base.
""")

# Initialize chat history in Streamlit session state if it doesn't exist
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Accept user input
if prompt := st.chat_input("Ask a question about the documents..."):
    # Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    # Display user message in chat message container
    with st.chat_message("user"):
        st.markdown(prompt)

    # Get response from RAG system
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                # Call the ask_rag_system function from rag_core.py
                response = rag_core.ask_rag_system(prompt)
                llm_answer = response["answer"]
                retrieved_context = response["context"]

                st.markdown(llm_answer) # Display the LLM's answer

                # Optionally display retrieved context (useful for debugging/transparency)
                if retrieved_context:
                    st.subheader("ðŸ“š Retrieved Context")
                    for i, doc in enumerate(retrieved_context):
                        source_info = f"Source: {doc.metadata.get('source', 'N/A')}"
                        page_info = f"Page: {doc.metadata.get('page', 'N/A')}"
                        st.expander(f"Document {i+1} ({source_info}, {page_info})").markdown(
                            f"```\n{doc.page_content}\n```"
                        )
                else:
                    st.info("No specific documents were retrieved for this query.")

                st.session_state.messages.append({"role": "assistant", "content": llm_answer})
            except Exception as e:
                st.error(f"An error occurred: {e}")
                st.session_state.messages.append({"role": "assistant", "content": f"Error: {e}"})


Phase 3: Run the RAG Web Application
This phase launches the interactive web application, allowing users to query your RAG system through a browser.
1.	Ensure your rag_nutanix_env is activated in your VS Code integrated terminal.
2.	Set NUTANIX_API_KEY (CRUCIAL for this session): In the same terminal session where you will run streamlit run, set the environment variable. This ensures the Streamlit process inherits the key.
o	For PowerShell:
o	$env:NUTANIX_API_KEY="YOUR_ACTUAL_NUTANIX_API_KEY_HERE"

o	For Command Prompt:
o	set NUTANIX_API_KEY=YOUR_ACTUAL_NUTANIX_API_KEY_HERE

(Replace YOUR_ACTUAL_API_KEY_HERE with your real Nutanix API key.)
o	Verify it's set in this session:
ï‚§	For PowerShell: $env:NUTANIX_API_KEY
ï‚§	For Command Prompt: echo %NUTANIX_API_KEY%
(It should print your key.)
3.	Navigate to your project directory in the terminal.
4.	Launch the Streamlit app:
5.	streamlit run streamlit_app.py

o	If streamlit is not recognized (i.e., you get an error like "The term 'streamlit' is not recognized..."), it means the streamlit.exe is not in your current terminal's PATH. You'll need to use its full path.
o	To find its full path, run this in a Jupyter cell in rag_system_full_workflow.ipynb:
o	import sys
o	import os
o	streamlit_exe_path = os.path.join(os.path.dirname(sys.executable), "Scripts", "streamlit.exe")
o	if not os.path.exists(streamlit_exe_path):
o	    # Fallback for common user-installed Python locations
o	    local_packages_dir = os.path.join(
o	        os.path.expanduser("~"), "AppData", "Local", "Packages",
o	        "PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0", # Adjust if your Python version is different
o	        "LocalCache", "local-packages", "Python312", "Scripts", "streamlit.exe"
o	    )
o	    if os.path.exists(local_packages_dir):
o	        streamlit_exe_path = local_packages_dir
o	    else:
o	        streamlit_exe_path = None
o	if streamlit_exe_path and os.path.exists(streamlit_exe_path):
o	    print(streamlit_exe_path)
o	else:
o	    print("Error: streamlit.exe path not found.")

o	Then, use the printed path with the call operator (&) in PowerShell:
o	& "C:\Users\JoshStevenson\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\Scripts\streamlit.exe" run streamlit_app.py

(Replace the path with your actual streamlit.exe path.)
6.	Streamlit will start a local web server and open your application in your default browser (usually http://localhost:8501).
5. Troubleshooting Common Issues
â€¢	conda: The term 'conda' is not recognized...: Ensure you are using the Anaconda Prompt, or that VS Code was launched from an Anaconda Prompt where conda is recognized. If persistent, consider a a clean re-install of Miniconda (see Project Setup - 1.b. Create and Activate the Conda Environment for initial installation steps and considerations during install).
â€¢	ModuleNotFoundError: No module named '...':
o	Verify the package is listed in your environment.yml.
o	Ensure the rag_nutanix_env is active ((rag_nutanix_env) in prompt).
o	Restart your VS Code and ensure the interpreter is correctly selected (Python 3.x.x ('rag_nutanix_env')).
â€¢	PermissionError: [WinError 5] Access is denied: 'my_vector_db': Close all instances of VS Code and any other programs accessing my_vector_db. Then, manually delete the my_vector_db folder using Windows File Explorer before re-running python ingest_data.py. This is critical on Windows.
â€¢	400 Bad Request from Embedding API (context length error): Your chunks are too long. Verify chunk_size in ingest_data.py is set to 400 characters (or less if needed), then re-run python ingest_data.py.
â€¢	404 Not Found from LLM API: Incorrect endpoint URL. Verify NUTANIX_LLM_ENDPOINT is set to https://ai.nutanix.com/api/v1 in rag_core.py.
â€¢	NUTANIX_API_KEY environment variable not set.: This is crucial. Ensure the NUTANIX_API_KEY is set in the same terminal session you are launching Streamlit from (using set NUTANIX_API_KEY=... for CMD or $env:NUTANIX_API_KEY=... for PowerShell) and before you run streamlit run. This variable is not automatically inherited by new processes without explicit action.
â€¢	streamlit : The term 'streamlit' is not recognized... or Unexpected token 'run' in expression or statement.: This means streamlit.exe is not in your PATH or PowerShell is having trouble parsing the direct execution. Use the full path to streamlit.exe with the & operator in PowerShell as detailed in Workflow Guide - Step 2.b. Action point 4.

